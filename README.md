# Federal-Register-Agentic-RAG-System

This project builds an Agentic Retrieval-Augmented Generation (RAG) system on top of the Federal Register API + MySQL + VectorDB using LangChain and LangGraph.

It allows you to:

+ ğŸ”„ Ingest & sync Federal Register documents into MySQL + VectorDB
+ ğŸ§  Query using natural language (e.g., "Show me Defense Department notices from July 2025")
+ ğŸ›  Run an agent that decides between SQL queries, VectorDB semantic search, and summarization
+ âš¡ Uses Gemini-2.0-flash (for reasoning and summarization

## âš™ï¸ Tech Stack

+ Backend

  - ğŸ¬ MySQL â†’ stores structured metadata (title, abstract, agencies, publication date)
  - ğŸ“¦ VectorDB (Chroma / Weaviate / Qdrant) â†’ semantic embeddings of abstracts + full text
  - ğŸ§‘â€ğŸ’» LangChain â†’ tool abstraction + agent orchestration
  - ğŸ”€ LangGraph â†’ stateful multi-step agent control

+ Pipeline

  - Daily sync from Federal Register API â†’ MySQL + VectorDB embeddings
  - Experiment tracking via MLflow (TODO)
  
## ğŸ“‚ Project Structure

```
federal-rag-agent/
â”‚â”€â”€ ingest/              # Pipeline: fetch docs, insert into MySQL & VectorDB
â”‚   â””â”€â”€ ingest_federal_register.py
â”‚â”€â”€ db/                  # Database schema + migrations
â”‚   â””â”€â”€ schema.sql
â”‚â”€â”€ agent/               # Agent logic (LangChain + LangGraph)
â”‚   â”œâ”€â”€ tools.py         # MySQL + VectorDB wrapped as LangChain tools
â”‚   â”œâ”€â”€ agent.py         # LangChain agent executor
â”‚   â””â”€â”€ graph_agent.py   # LangGraph controlled agent
â”‚â”€â”€ notebooks/           # Exploration / dev
â”‚â”€â”€ README.md
```

## ğŸš€ Setup
## 1ï¸âƒ£ Clone & Install
```
git clone https://github.com/<your-username>/federal-rag-agent.git
cd federal-rag-agent

pip install -r requirements.txt
```
requirements.txt : 
```
langchain
langgraph
langchain-community
mysql-connector-python
chromadb    # or weaviate-client / qdrant-client
ollama
```
## 2ï¸âƒ£ Setup MySQL

```
CREATE DATABASE federal_register;
USE federal_register;

CREATE TABLE documents (
    id INT AUTO_INCREMENT PRIMARY KEY,
    document_number VARCHAR(50) UNIQUE,
    title TEXT,
    abstract TEXT,
    publication_date DATE,
    agency_names TEXT,
    html_url TEXT,
    pdf_url TEXT
);
```
## 3ï¸âƒ£ Setup VectorDB

Example: ChromaDB (local)
```
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
vectordb = Chroma(persist_directory="./chroma_store", embedding_function=embeddings) 
```
Each documentâ€™s abstract + title is embedded into the store.

## 4ï¸âƒ£ Run Ingestion
```
python ingest/ingest_federal_register.py
```
This pulls documents from the Federal Register API, stores metadata in MySQL, and embeddings in VectorDB.

## 5ï¸âƒ£ Run Agent (LangChain)
```
response = agent.run("Show me the most recent Defense Department notices from July 2025")
print(response)
```
## 6ï¸âƒ£ Run Agent (LangGraph)
```
python agent/graph_agent.py
```
LangGraph allows custom state machines â†’ e.g.,

+ Step 1: Decide if SQL lookup or semantic search needed
+ Step 2: Call MySQL / VectorDB tool
+ Step 3: Summarize with LLM

## ğŸ§° Available Tools

+ search_documents(keyword, start_date, end_date) â†’ SQL query in MySQL
+ get_recent_documents(days) â†’ fetch latest docs
+ summarize_document(doc_id) â†’ get abstract/title
+ semantic_search(query, k=5) â†’ search in VectorDB

## ğŸ›  Next Steps

+ Including asynchronous operations
+ Add streaming answers (FastAPI + WebSocket frontend)
+ Add fine-tuned summarization model for legal texts
+ Extend ingestion pipeline for daily auto-update via cron
+ Multi-agent LangGraph (SQL agent + RAG agent + Summarizer agent)
